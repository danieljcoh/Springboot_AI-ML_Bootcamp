{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "435d1410",
   "metadata": {},
   "source": [
    "# Churn Mini Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a46f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q boto3 sagemaker pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df87b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, sagemaker, time, os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "# ---- Configure AWS (EDIT THESE IF YOU'RE NOT IN SAGEMAKER STUDIO) ----\n",
    "AWS_REGION = os.environ.get(\"AWS_REGION\", \"us-east-1\")\n",
    "SESSION = boto3.session.Session(region_name=AWS_REGION)\n",
    "S3 = SESSION.resource(\"s3\")\n",
    "S3_CLIENT = SESSION.client(\"s3\")\n",
    "SM_SESSION = sagemaker.Session(boto_session=SESSION)\n",
    "\n",
    "# Role: inside SageMaker, this picks up the execution role automatically.\n",
    "try:\n",
    "    from sagemaker import get_execution_role\n",
    "    ROLE = get_execution_role()\n",
    "except Exception:\n",
    "    # If running locally, set your IAM role ARN with SageMaker permissions\n",
    "    ROLE = \"arn:aws:iam::<YOUR-AWS-ACCOUNT-ID>:role/<YOUR-SAGEMAKER-ROLE-NAME>\"  # <-- EDIT\n",
    "\n",
    "# S3 bucket: create or reuse\n",
    "ACCOUNT_ID = SESSION.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "default_bucket = SM_SESSION.default_bucket()\n",
    "BUCKET = os.environ.get(\"S3_BUCKET\", default_bucket)  # reuse SageMaker default if available\n",
    "print(\"Using S3 bucket:\", BUCKET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple, synthetic features for demo; replace with your real data if you have it.\n",
    "rng = np.random.default_rng(42)\n",
    "N = 20000\n",
    "df = pd.DataFrame({\n",
    "    \"tenure_months\": rng.integers(1, 60, size=N),\n",
    "    \"monthly_charges\": rng.normal(65, 25, size=N).clip(5, 200),\n",
    "    \"total_charges\": lambda x: (x[\"tenure_months\"] * x[\"monthly_charges\"]).clip(10, 10000),\n",
    "    \"support_calls_last_90d\": rng.poisson(1.8, size=N).clip(0, 20),\n",
    "    \"is_promo\": rng.integers(0, 2, size=N),\n",
    "    \"contract_type\": rng.integers(0, 3, size=N),  # 0=month-to-month, 1=1yr, 2=2yr\n",
    "})\n",
    "\n",
    "# Churn: higher with short tenure, high calls, non-promo, month-to-month\n",
    "logit = (\n",
    "    -2.0\n",
    "    + 0.03*(200 - df[\"monthly_charges\"].values)/10\n",
    "    + 0.08*(5 - df[\"tenure_months\"].values)/2\n",
    "    + 0.35*df[\"support_calls_last_90d\"].values\n",
    "    + 0.25*(1 - df[\"is_promo\"].values)\n",
    "    + 0.30*(df[\"contract_type\"].values == 0).astype(float)\n",
    ")\n",
    "p = 1 / (1 + np.exp(-logit))\n",
    "df[\"churn\"] = (rng.random(N) < p).astype(int)\n",
    "\n",
    "# Train/val/test split\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=8, stratify=df[\"churn\"])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=8, stratify=temp_df[\"churn\"])\n",
    "\n",
    "print(\"Train/Val/Test sizes:\", len(train_df), len(val_df), len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost expects label in first column for CSV input\n",
    "def to_smxgb_csv(d: pd.DataFrame, label_col=\"churn\"):\n",
    "    cols = [label_col] + [c for c in d.columns if c != label_col]\n",
    "    return d[cols]\n",
    "\n",
    "prefix = \"churn-simple-xgb\"\n",
    "local_train = \"train.csv\"; local_val = \"validation.csv\"; local_test = \"test.csv\"\n",
    "\n",
    "to_smxgb_csv(train_df).to_csv(local_train, index=False, header=False)\n",
    "to_smxgb_csv(val_df).to_csv(local_val, index=False, header=False)\n",
    "to_smxgb_csv(test_df).to_csv(local_test, index=False, header=False)\n",
    "\n",
    "train_s3 = f\"s3://{BUCKET}/{prefix}/data/{local_train}\"\n",
    "val_s3   = f\"s3://{BUCKET}/{prefix}/data/{local_val}\"\n",
    "test_s3  = f\"s3://{BUCKET}/{prefix}/data/{local_test}\"\n",
    "\n",
    "S3_CLIENT.upload_file(local_train, BUCKET, f\"{prefix}/data/{local_train}\")\n",
    "S3_CLIENT.upload_file(local_val,   BUCKET, f\"{prefix}/data/{local_val}\")\n",
    "S3_CLIENT.upload_file(local_test,  BUCKET, f\"{prefix}/data/{local_test}\")\n",
    "\n",
    "print(\"Uploaded:\")\n",
    "print(train_s3)\n",
    "print(val_s3)\n",
    "print(test_s3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_image = image_uris.retrieve(framework=\"xgboost\", region=AWS_REGION, version=\"1.5-1\")\n",
    "output_path = f\"s3://{BUCKET}/{prefix}/output\"\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=ROLE,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=SM_SESSION,\n",
    ")\n",
    "\n",
    "# Reasonable starter hyperparameters (tweak later)\n",
    "xgb.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    max_depth=6,\n",
    "    eta=0.2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    num_round=200,\n",
    "    min_child_weight=1,\n",
    "    verbosity=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90709589",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_inputs = {\n",
    "    \"train\": sagemaker.inputs.TrainingInput(train_s3, content_type=\"text/csv\"),\n",
    "    \"validation\": sagemaker.inputs.TrainingInput(val_s3, content_type=\"text/csv\"),\n",
    "}\n",
    "xgb.fit(s3_inputs, logs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d865a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"churn-xgb-{int(time.time())}\"\n",
    "predictor = xgb.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=endpoint_name,\n",
    ")\n",
    "\n",
    "# CSV in/out serializer\n",
    "predictor.serializer = CSVSerializer()\n",
    "predictor.deserializer = CSVDeserializer()\n",
    "print(\"Endpoint:\", endpoint_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b45533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test without label for inference (drop 'churn')\n",
    "test_features = test_df.drop(columns=[\"churn\"])\n",
    "# XGBoost expects features only for prediction, in the same order used for training\n",
    "feature_order = [c for c in to_smxgb_csv(train_df).columns if c != \"churn\"]\n",
    "test_matrix = test_features[feature_order].astype(float)\n",
    "\n",
    "# Take a small sample to invoke\n",
    "sample = test_matrix.head(10)\n",
    "payload = \"\\n\".join([\",\".join(map(str, row)) for row in sample.values.tolist()])\n",
    "\n",
    "preds = predictor.predict(payload)\n",
    "# preds is list of lists like [['0.123'], ['0.876'], ...]\n",
    "scores = np.array([float(p[0]) for p in preds])\n",
    "pd.DataFrame({\"score\": scores})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "# Score a larger chunk (e.g., 500 rows)\n",
    "eval_chunk = test_matrix.head(500)\n",
    "true_y = test_df[\"churn\"].head(500).values\n",
    "payload = \"\\n\".join([\",\".join(map(str, row)) for row in eval_chunk.values.tolist()])\n",
    "preds = predictor.predict(payload)\n",
    "scores = np.array([float(p[0]) for p in preds])\n",
    "\n",
    "auc = roc_auc_score(true_y, scores)\n",
    "aps = average_precision_score(true_y, scores)\n",
    "brier = brier_score_loss(true_y, scores)\n",
    "print(f\"AUC: {auc:.4f} | AP: {aps:.4f} | Brier: {brier:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83abba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "    print(\"Deleted endpoint:\", endpoint_name)\n",
    "except Exception as e:\n",
    "    print(\"Cleanup error:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
